{
  "timestamp": "2025-08-23 01:23",
  "results": {
    "overall_accuracy": 0.7287454314338493,
    "accuracy_by_level": {
      "<8": {
        "acc": 0.7524354087251165,
        "n": 2361,
        "lo": 0.7346251237459699,
        "hi": 0.7694255492462508
      },
      "8-10": {
        "acc": 0.756532519549876,
        "n": 15729,
        "lo": 0.7497632291212193,
        "hi": 0.7631765312333247
      },
      "10-12": {
        "acc": 0.7552531737924997,
        "n": 95942,
        "lo": 0.7525224402326879,
        "hi": 0.7579634670585772
      },
      "12-13": {
        "acc": 0.745359452263218,
        "n": 105160,
        "lo": 0.7427173610220831,
        "hi": 0.7479836177067122
      },
      "13-14": {
        "acc": 0.7298893671224153,
        "n": 97349,
        "lo": 0.7270910714423227,
        "hi": 0.7326695196650663
      },
      "14-15": {
        "acc": 0.6525660459073191,
        "n": 46180,
        "lo": 0.6482106410165723,
        "hi": 0.6568960697253076
      },
      "15+": {
        "acc": 0.6409438775510204,
        "n": 15680,
        "lo": 0.6334013315111205,
        "hi": 0.6484173780070884
      }
    },
    "utr_distribution": {
      "mean": 12.63191360487948,
      "median": 12.75,
      "min": 1.38,
      "max": 16.45
    },
    "total_matches": 378401,
    "ties": 1860,
    "regression_results": [
      {
        "Level": "All",
        "Matches": 185764,
        "Coefficient": 1.4964003040877358,
        "Intercept": 0.0760933417756487,
        "Accuracy": 0.7252912297323486,
        "Brier": 0.1793004959898832,
        "AUC": 0.7946820342728421
      },
      {
        "Level": "UTR < 12",
        "Matches": 53359,
        "Coefficient": 1.40453411932625,
        "Intercept": 0.17179354706466815,
        "Accuracy": 0.7902884236960962,
        "Brier": 0.14415377850476366,
        "AUC": 0.8404959004242398
      },
      {
        "Level": "UTR 12-14",
        "Matches": 106917,
        "Coefficient": 1.5826682119940823,
        "Intercept": 0.04407974923273123,
        "Accuracy": 0.7165090677815502,
        "Brier": 0.1848085479202863,
        "AUC": 0.7856006524373006
      },
      {
        "Level": "UTR 14+",
        "Matches": 25488,
        "Coefficient": 1.4711001061273,
        "Intercept": 0.0002132470842132765,
        "Accuracy": 0.6242937853107344,
        "Brier": 0.2289254410545191,
        "AUC": 0.6707780603464569
      }
    ],
    "calibration": {
      "brier": 0.1793004959898832,
      "ece": 0.010817628642226941,
      "mce": 0.021988551958643243
    }
  },
  "plots_generated": [
    "win_rate_by_utr_diff.png",
    "win_rate_by_small_utr_diff.png",
    "accuracy_by_utr_level.png",
    "accuracy_heatmap.png",
    "count_heatmap.png",
    "logistic_regression_curve.png",
    "roc_curve.png",
    "calibration_curve.png",
    "utr_distribution.png",
    "utr_distribution_by_range.png",
    "logistic_regression_by_level.png",
    "accuracy_by_year.png"
  ]
}